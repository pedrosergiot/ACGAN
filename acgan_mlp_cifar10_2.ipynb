{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"acgan_mlp_cifar10_2.ipynb","provenance":[{"file_id":"1A1QQyYeCLmcNWdMp2SeYjdPMXDsiVl4T","timestamp":1569266020080}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rDuVfKYkJh4l","colab_type":"code","outputId":"96dbd773-53cb-40dc-c15f-f730ef62d45a","executionInfo":{"status":"ok","timestamp":1569270696703,"user_tz":180,"elapsed":4128562,"user":{"displayName":"Pedro SÃ©rgio","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNOQ8qokex1JEcP530XFUpBefCchi0RJaiVj_ahw=s64","userId":"09920799442367656805"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1qN3izAhIpWNKuHtN2a5se5HdlvUSCvb0"}},"source":["from __future__ import print_function, division\n","\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n","from tensorflow.keras.layers import BatchNormalization, Activation, Embedding\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.initializers import RandomNormal\n","\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","class ACGAN():\n","    def __init__(self):\n","        # Input shape\n","        self.img_rows = 32\n","        self.img_cols = 32\n","        self.channels = 3\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.num_classes = 10\n","        self.latent_dim = 110\n","\n","        optimizer = Adam(0.0002, 0.5)\n","        losses = ['binary_crossentropy', 'sparse_categorical_crossentropy']\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss=losses,\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","        \n","        self.discriminator.summary()\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","        \n","        self.generator.summary()\n","\n","        # The generator takes noise and the target label as input\n","        # and generates the corresponding digit of that label\n","        noise = Input(shape=(self.latent_dim,))\n","        label = Input(shape=(1,))\n","        img = self.generator([noise, label])\n","        \n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated image as input and determines validity\n","        # and the label of that image\n","        valid, target_label = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model([noise, label], [valid, target_label])\n","        self.combined.compile(loss=losses,\n","            optimizer=optimizer)\n","\n","    def build_generator(self):\n","\n","        init = RandomNormal(stddev=0.02)\n","\n","        model = Sequential()\n","\n","        model.add(Dense(256, input_dim=self.latent_dim, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization())\n","        model.add(Dense(512, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization())\n","        model.add(Dense(1024, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization())\n","        model.add(Dense(2048, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization())\n","        model.add(Dense(np.prod(self.img_shape), activation='tanh', kernel_initializer=init))\n","        model.add(Reshape(self.img_shape))\n","\n","        model.summary()\n","\n","        noise = Input(shape=(self.latent_dim,))\n","        label = Input(shape=(1,), dtype='int32')\n","        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n","\n","        model_input = multiply([noise, label_embedding])\n","        img = model(model_input)\n","        \n","        return Model([noise, label], img)\n","\n","    def build_discriminator(self):\n","\n","        init = RandomNormal(stddev=0.02)\n","        \n","        model = Sequential()\n","\n","        model.add(Flatten(input_shape=self.img_shape))\n","        model.add(Dense(1024, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dense(512, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dense(256, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","\n","        model.summary()\n","\n","        img = Input(shape=self.img_shape)\n","\n","        # Extract feature representation\n","        features = model(img)\n","\n","        # Determine validity and label of the image\n","        validity = Dense(1, activation=\"sigmoid\")(features)\n","        label = Dense(self.num_classes, activation=\"softmax\")(features)\n","                \n","        return Model(img, [validity, label])\n","\n","    def train(self, epochs, batch_size=128, sample_interval=50):\n","        \n","        (X_train, y_train), (_, _) = cifar10.load_data()\n","\n","        # Configure inputs\n","        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n","        y_train = y_train.reshape(-1, 1)\n","\n","        # Adversarial ground truths\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","        \n","        D_loss = []\n","        G_loss = []\n","        Acc = []\n","\n","        for epoch in range(epochs):\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            # Select a random batch of images\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","\n","            # Sample noise as generator input\n","            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","\n","            # The labels of the digits that the generator tries to create an\n","            # image representation of\n","            sampled_labels = np.random.randint(0, 10, (batch_size, 1))\n","\n","            # Generate a half batch of new images\n","            gen_imgs = self.generator.predict([noise, sampled_labels])\n","\n","            # Image labels. 0-9 \n","            img_labels = y_train[idx]\n","\n","            # Train the discriminator\n","            d_loss_real = self.discriminator.train_on_batch(imgs, [valid, img_labels])\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [fake, sampled_labels])\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","\n","            # Train the generator\n","            g_loss = self.combined.train_on_batch([noise, sampled_labels], [valid, sampled_labels])\n","\n","            # Plot the progress\n","            print (\"%d [D loss: %f, acc.: %.2f%%, op_acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss[0]))\n","\n","            D_loss.append(d_loss[0])\n","            G_loss.append(g_loss[0])\n","            Acc.append(d_loss[3])\n","            \n","            # If at save interval => save generated image samples\n","            if epoch % sample_interval == 0:\n","                self.save_model()\n","                self.sample_images(epoch)\n","        return D_loss, G_loss, Acc\n","\n","    def sample_images(self, epoch):\n","        r, c = 5, 5\n","        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","        sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n","        gen_imgs = self.generator.predict([noise, sampled_labels])\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","        \n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt,:,:,:])\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(\"/content/gdrive/My Drive/Deep Learning/acgan_mlp_cifar10_2/images/%d.png\" % epoch)\n","        plt.close()\n","\n","    def save_model(self):\n","\n","        def save(model, model_name):\n","            model_path = \"/content/gdrive/My Drive/Deep Learning/acgan_mlp_cifar10_2/saved_model/%s.json\" % model_name\n","            weights_path = \"/content/gdrive/My Drive/Deep Learning/acgan_mlp_cifar10_2/saved_model/%s_weights.hdf5\" % model_name\n","            options = {\"file_arch\": model_path,\n","                        \"file_weight\": weights_path}\n","            json_string = model.to_json()\n","            open(options['file_arch'], 'w').write(json_string)\n","            model.save_weights(options['file_weight'])\n","\n","        save(self.generator, \"generator\")\n","        save(self.discriminator, \"discriminator\")\n","\n","\n","if __name__ == '__main__':\n","    acgan = ACGAN()\n","    D_loss, G_loss, Acc = acgan.train(epochs=50000, batch_size=100, sample_interval=200)"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}